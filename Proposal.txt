For the purpose of my Final Year Project (FYP) I would like to propose an implementation of a language that is inspired by Dijkstras Guarded Command Language (GCL). The language aims to incorporate what Dijkstra set out in GCL, a language which produces formally verified programs based on the more mathematical apporach to designing programs. The proposal will be split into a few sections namely, why I chose GCL as the foundation for my language and what makes GCL interesting.


## On Choosing GCL

Some of my reasons for choosing GCL do contain a slight personal element to them. Superficially, GCL allows you to approach programming much more like an exercise in mathematics than an exercise in of itself. In his paper EWD 361 Programming As A Discipline Of Mathematical Nature, Dijkstra notes there emerged a crisis based around the question of how can we rely on our algorithms because after all, the program that passes n unit tests could fail on the 'n + 1' unit test. What came after this realisation were proof methods that "were so long, hairy and clumsy, that they failed to convince." They only furthered to increase the level of skepticism that people had for the proofs however a few discoveries were made since then. 

The first is that the labour of proving the correctness of a program depends heavily on the structure of a program. This highlights an important aspect of GCL. A non-trivial program is one in which the programmer decreases a variant while mainiting the invariant until the guard is evaluated to false, in other words, a loop. This is not to say trivial programs are those that are not loops but all programs in GCL are done more or less with the purpose of a loop in mind. The reason for this is that since the proof of a program relies on the structure of the program, all programs must share the same base structure in order to allow for consistency in the proofs. 

The third discovery is that setting out to prove a program is "putting the cart before the horse". Instead, one should allow the program and the proof work in tandem to the goal of correctness. In fact correctness becomes not only the goal of the proof but becomes a guiding hand in the proof itself. In personal experience, this was a revelation for me. It dymystified what programming is at its heart which when following the GCL paradigm, is an exercise of formal methods. To demonstrate how the proof and the program guide eachother, consider the following program which sums the elements in an array.

* Let f be an array of size N containing N amount of unordered integers where N is an integer.
* Our post-condition would look something like the following: < + j : 0 <= j < N : f.j >
* What this structure is informing us of is that we operating between the 0th element of the array and the (n - 1)th. No predicate is being applied in this program which we know from the 'f.j' article. A predicate P could applied to it in cases of seeing if the number is odd or prime for example. The first article '+ j' tells us that we are applying the operator addition across this array and that we are focused on a single element of the array at a time. All of this lessons the ambiguity that we may have about the task at hand. 

* First we start with the Model. 

Model
* (0) C.n = < + j : 0 <= j < n : f.j > ^ n = N      -- Our first axiom is the post condition of the program. Axioms will be denoted by * and theorems by O. All theorems are dervied from the axioms which are given to us in the program model.
O (1) C.0 = 0					    -- The first thing that we do is to see what value the program has when we apply the theorem of "Empty Range" or in other cases "False Range"
O (2) C.(n + 1) = C.n + f.n                         -- The second theorem is the second instance of letting the proof and the program guide us to correctness. In the first theorem, the natural step is to check what the value of 'C.0' when we know what 'C.n' is already. If we know what 'C.0' is then the second step is to check what the value is when we iterate from zero to one, or in more general terms, from n to n + 1. The result is the previous state of 'C.n' plus the value at the next iteration of n at the array. 

* The next thing that we do is rewrite the post condition in terms of the model.

r = C.n 

* The next thing that we derive are the invariants, the variant, and the guard of the program. 

Invariants
P0 : r = C.n 					    -- Since we know that the invariant cannot change and we know that C.n doesn't change, we can propose C.n to be the invariant.
P1 : 0 <= n < N 				    -- The second variant are the ranges that we are concerned with because we're not looking to go below or over our bounds. Therefore they are invariant as they do not change. 

Variant
N - n                                               -- Another way to think of the variant is that it's a measure of how much work is left to do in the program. This leads to the obvious expression of 'N - n'.

Guard 
n != N                                              -- Since we are decreasing the variant upon every iteration of our loop, we know that once the variant holds the value of the upper-bound or the lower-bound then we are finished. If it holds this value, the program stops which implies that for the program to even begin, it must not hold this value which leads to the expression 'n != N'.

* Next we establish the invariants through setting n and r to the same value. This value is zero for n and the value of 'C.0' for r as 'C.n' is influenced by the value of n and since there is no way to start the program based on the Model from any other value but zero.

* Next we perform an iteration to see how state changes throughout the program. This step is simply called Loop Body.

Loop Body

	(n, r := n + 1, E).P0   
=       { Textual Substitution }     -- This step maintains the invariant. 'r' is set to 'E' because we do not know how it changes yet.
        E = C.(n + 1)                -- Since 'r' is 'C.n', we know that when n is increased 'r' has to follow by the same increase.
=       { (2) }                      -- Our Model now tells us what 'C.(n + 1)' refers to and offers us an equivalent expression which allows us to proceed further.
        E = C.n + f.n                
=       { P0  }                      -- We now look back through the proof and note that the post condition of our program has been rewritten.
        E = r + f.n                  -- We now know what E is in terms of 'r' can say that the expression E refers to is how 'r' behaves throughout the program. 

* And now all that is left to write the program which due to the proof construction is nearly arbitrary. 

n, r := 0, 0

Do n != N ->
	    n, r := n + 1, r + f.n 
Od

I hope my comments to the construction of the program show how the correctness can guide the proof. This in my opinion is simply a beautiful way to program. It leads to more elegant solutions that are tremendously more optimised than if one were to come up with the program in other ways. It for one reduces the amount of bugs in a program. To refer back to the paper, Dijkstra lamented about how in a lot of instances, programmers after having writtten a program, come up with the theory after the fact. Deriving a program in the way above seems like a much better approach to me. It allows for more elegant code and it abstracts away the complexities of algorithm design and optimisation to something that is entirely methodical in nature. Although highly methodical, non-determinism arrises does arise in some instances.

In his paper "Guarded commands. non-determinacy and a calculus for the derivation of programs", he gives an example of the non-determinacy. Consider the program that assigns to the variables q1, q2, q3 and q4 a permutation of the values Q1, Q2, Q3 and Q4, such that q1 ≤ q2 ≤ q3 ≤ q4. 

q1, q2, q3, q4 := Q1, Q2, Q3, Q4
Do q1 > q2 -> q1, q2 := q2, q1
[] q2 > q3 -> q2, q3 := q3, q2
[] q3 > q4 -> q3, q4 := q4, q3
Od

I think an obvious problem about this is the overhead (which is dependent on the specific program of course). Disregarding the problems that can arise due to non-determinancy, non-determinancy is a really interesting effect considering the methodology of deriving a GCL progrm is rigorous. If we were to consider a program to be a graph wherein the nodes of the graph are the expressions of the program. If we were now to consider this graph as a NFA graph, performing a Breadth First Search in the following way could simulate non-determinancy.
	Start with a single execution thread from the starting state which would be 'Do GUARD ->' state.
	Whenever one or more possible options occur, produce a new execution thread for each option.
	Keep all of these threads in a queue and iterate over them in a Round Robin way. 

We could achieve non-determinacy in other ways however in other ways. Since non-determinacy only arises when a program contains selection, if we encounter a program involving selection, we can implement concurrency. We achieve non-determinacy by giving considering the selection state as a tuple (selection state, value) where value is an integer that refers to the probability of being picked. When the selection choices our encountered, we are able to pick such a selection at random and adjust its value accordingly so it's less likely to picked again. This is probably the best way to optimize "randomness" that isn't just letting the choices being picked at random by the language of implementation. 

The language that I am propsing is thusly one that is very much like GCL but is extended in a lot of ways. The most notable selection is the inclusion of function definitions. I feel like this is a very natural step forward with GCL because it allows us to do more useful things in the language. The second is the inclusion of recursion. Some algorithms are inherently recursive and I think the strictness of disallowing recursion does a disservice to these algorithms. The third is since the Model is written in a functional style, the data structures of the FYP should be immutable. 

Regarding implementation, there are two paths that are open to the development of this language. The first is an implementation in Haskell. The second is using the LLVM tools with Python. I am inclined to choose Haskell due to the type safety and the lack of ambiguity but if development should prove too slow, I think there is possibility that I may have to switch to Python. 



